{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "import org.apache.spark.sql.functions._\n",
        "import org.apache.spark.sql.{DataFrame, SaveMode, SparkSession}\n",
        "import org.apache.spark.sql.Row\n",
        "import org.apache.spark.sql.types.{StructType, StructField, LongType}\n",
        "import spark.implicits._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "val spark = SparkSession.builder().appName(\"tlcYellowETL\").getOrCreate()\n",
        "\n",
        "\n",
        "def loadRawTLCDataParquet(spark: SparkSession, path: String): DataFrame= {\n",
        "    spark.read.parquet(s\"$path.parquet\")\n",
        "  }\n",
        " \n",
        "  \n",
        "def saveCleanData(resultDF: DataFrame, cleanOutputPath: String): Unit = {\n",
        "    resultDF.write.mode(SaveMode.Overwrite).parquet(s\"$cleanOutputPath/merged_yellow_cleaned_data.parquet\")\n",
        "  }\n",
        "  \n",
        "  \n",
        "def loadCleanData(path: String): DataFrame = {\n",
        "    spark.read.parquet(f\"$path/merged_yellow_cleaned_data.parquet\")\n",
        "  }\n",
        "  \n",
        "\n",
        "def getNumericColumnLowerAndUpperBound(df: DataFrame, columnName: String) = {\n",
        "    val meanValue = df1.agg(stddev(columnName).alias(\"stddev\"), mean(columnName).alias(\"mean\")).head()\n",
        "    val stddev_ = meanValue.getDouble(0)\n",
        "    val mean_ = meanValue.getDouble(1)\n",
        "    // Define the upper and lower bounds\n",
        "    val lowerBound = mean_ - 3 * stddev_\n",
        "    val upperBound = mean_ + 3 * stddev_\n",
        "    (lowerBound, upperBound)\n",
        "}\n",
        "\n",
        "\n",
        "def cleanRawData(rawDF: DataFrame): DataFrame = {\n",
        "\n",
        "    // Keep non-null entries in these columns\n",
        "    val nonNullDF = rawDF.filter(\n",
        "        col(\"PUlocationID\").isNotNull &&\n",
        "        col(\"DOlocationID\").isNotNull &&\n",
        "        col(\"total_amount\").isNotNull &&\n",
        "        col(\"fare_amount\").isNotNull &&\n",
        "        col(\"payment_type\").isNotNull &&\n",
        "        col(\"RatecodeID\").isNotNull)\n",
        "    \n",
        "    val newColumnNames = Map(\n",
        "      \"tpep_dropoff_datetime\" -> \"dropoff_datetime\",\n",
        "      \"tpep_pickup_datetime\" -> \"pickup_datetime\",\n",
        "      \"PUlocationID\" -> \"pulocationID\",\n",
        "      \"DOlocationID\" -> \"dolocationID\"\n",
        "    )\n",
        "        \n",
        "    val renamedDF = newColumnNames.foldLeft(nonNullDF) {\n",
        "        case (accDF, (oldName, newName)) => accDF.withColumnRenamed(oldName, newName)\n",
        "    }\n",
        "    \n",
        "    // Compute valid ranges of values for columns\n",
        "    val validPaymentTypes = Seq(1, 2, 3)\n",
        "    val validRateCodeIds = Seq(1, 5, 6)\n",
        "    val validTotalAmount = getNumericColumnLowerAndUpperBound(rawDF, \"total_amount\")\n",
        "    val totalAmountLower = validTotalAmount._1\n",
        "    val totalAmountUpper = validTotalAmount._2\n",
        "    \n",
        "    // Add month-date from data and compare it with month-date extracted from data file name\n",
        "    val transformedDF = renamedDF.withColumn(\"year_month\", regexp_extract(col(\"file_path\"), pattern.toString, 1)).withColumn(\"data_year_month\", concat_ws(\"-\", year(col(\"pickup_datetime\")), format_string(\"%02d\", month(col(\"pickup_datetime\")))))\n",
        "    \n",
        "    // Remove rows which follow rules\n",
        "    val filteredDF = transformedDF.filter(col(\"passenger_count\") =!= 0).filter(col(\"pulocationID\") =!= col(\"dolocationID\")).filter(col(\"data_year_month\") === col(\"year_month\")).filter(col(\"payment_type\").isin(validPaymentTypes: _*)).filter(col(\"RatecodeID\").isin(validRateCodeIds: _*)).filter(col(\"fare_amount\") =!= 0).filter(col(\"total_amount\").between(totalAmountLower, totalAmountUpper))\n",
        "    \n",
        "    // Impute: cast to Long\n",
        "    val castDf = filteredDF.withColumn(\"pulocationID\", col(\"pulocationID\").cast(LongType)).withColumn(\"dolocationID\", col(\"dolocationID\").cast(LongType))\n",
        "    castDf.select(\"pulocationID\", \"dolocationID\").coalesce(1)\n",
        "}\n",
        "  \n",
        " \n",
        "def ETLYellowTaxiTripDataset(spark: SparkSession, path: String, cleanOutputPath:String): Unit = {\n",
        "    val years = Array(\"2020\", \"2021\", \"2022\", \"2023\")\n",
        "    val months = Array(\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\")\n",
        "    \n",
        "    val filePaths2020 = for (m <- months.slice(9, 12)) yield s\"$path/2020/$m/yellow_tripdata_2020-${m}\"\n",
        "    val filePaths2021 = for (m <- months) yield s\"$path/2021/${m}/yellow_tripdata_2021-${m}\"\n",
        "    val filePaths2022 = for (m <- months) yield s\"$path/2022/${m}/yellow_tripdata_2022-${m}\"\n",
        "    val filePaths2023 = for (m <- months.slice(0, 9)) yield s\"$path/2023/${m}/yellow_tripdata_2023-${m}\"\n",
        "    \n",
        "    val filePaths = filePaths2020 ++ filePaths2021 ++ filePaths2022 ++ filePaths2023\n",
        "    \n",
        "    val pattern = \"\"\"(\\d{4}-\\d{2})\"\"\".r\n",
        "    \n",
        "    val schema = StructType(Array(StructField(\"pulocationID\", LongType, true), StructField(\"dolocationID\", LongType, true)))\n",
        "    var resultDF = spark.createDataFrame(spark.sparkContext.emptyRDD[Row], schema)\n",
        "\n",
        "    for (filePath <- filePaths.slice(0, 1)) {\n",
        "        val rawDF = loadRawTLCDataParquet(spark, filePath)\n",
        "        val transformedDF = rawDF.withColumn(\"file_path\", lit(filePath))\n",
        "        val cleanDF = cleanRawData(transformedDF)\n",
        "        resultDF = resultDF.union(cleanDF)\n",
        "    }\n",
        "    \n",
        "    saveCleanData(resultDF, cleanOutputPath)\n",
        "    loadCleanData(cleanOutputPath).show()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "ETLYellowTaxiTripDataset(spark, \"/user/cg4177_nyu_edu/project/data/source/tlc/yellow\", \"/user/cg4177_nyu_edu/project/data/clean/tlc/yellow\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Profiling for TLC Yellow Taxi Trip Record Data\n",
        "\n",
        "Four options:\n",
        "* Number of Distinct Values\n",
        "* Value Counts\n",
        "* Frequency Distribution\n",
        "* Mode "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "val tlcYellowCleanDF = loadCleanData(\"/user/cg4177_nyu_edu/project/data/clean/tlc/yellow\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "z.show(tlcYellowCleanDF.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "val spark = SparkSession.builder().appName(\"tlcGreenETL\").getOrCreate()\n",
        "\n",
        "\n",
        "def loadRawTLCDataParquet(spark: SparkSession, path: String): DataFrame= {\n",
        "    spark.read.parquet(s\"$path.parquet\")\n",
        "  }\n",
        " \n",
        "  \n",
        "def saveCleanData(resultDF: DataFrame, cleanOutputPath: String): Unit = {\n",
        "    resultDF.write.mode(SaveMode.Overwrite).parquet(s\"$cleanOutputPath/merged_green_cleaned_data.parquet\")\n",
        "  }\n",
        "  \n",
        "  \n",
        "def loadCleanData(path: String): DataFrame = {\n",
        "    spark.read.parquet(f\"$path/merged_green_cleaned_data.parquet\")\n",
        "  }\n",
        "  \n",
        "\n",
        "def getNumericColumnLowerAndUpperBound(df: DataFrame, columnName: String) = {\n",
        "    val meanValue = df1.agg(stddev(columnName).alias(\"stddev\"), mean(columnName).alias(\"mean\")).head()\n",
        "    val stddev_ = meanValue.getDouble(0)\n",
        "    val mean_ = meanValue.getDouble(1)\n",
        "    // Define the upper and lower bounds\n",
        "    val lowerBound = mean_ - 3 * stddev_\n",
        "    val upperBound = mean_ + 3 * stddev_\n",
        "    (lowerBound, upperBound)\n",
        "}\n",
        "\n",
        "\n",
        "def cleanRawData(rawDF: DataFrame): DataFrame = {\n",
        "\n",
        "    // Keep non-null entries in these columns\n",
        "    val nonNullDF = rawDF.filter(\n",
        "        col(\"PUlocationID\").isNotNull &&\n",
        "        col(\"DOlocationID\").isNotNull &&\n",
        "        col(\"total_amount\").isNotNull &&\n",
        "        col(\"fare_amount\").isNotNull &&\n",
        "        col(\"payment_type\").isNotNull &&\n",
        "        col(\"RatecodeID\").isNotNull)\n",
        "    \n",
        "    val newColumnNames = Map(\n",
        "      \"lpep_pickup_datetime\" -> \"dropoff_datetime\",\n",
        "      \"lpep_dropoff_datetime\" -> \"pickup_datetime\",\n",
        "      \"PUlocationID\" -> \"pulocationID\",\n",
        "      \"DOlocationID\" -> \"dolocationID\"\n",
        "    )\n",
        "        \n",
        "    val renamedDF = newColumnNames.foldLeft(nonNullDF) {\n",
        "        case (accDF, (oldName, newName)) => accDF.withColumnRenamed(oldName, newName)\n",
        "    }\n",
        "    \n",
        "    // Compute valid ranges of values for columns\n",
        "    val validPaymentTypes = Seq(1, 2, 3)\n",
        "    val validRateCodeIds = Seq(1, 5, 6)\n",
        "    val validTotalAmount = getNumericColumnLowerAndUpperBound(rawDF, \"total_amount\")\n",
        "    val totalAmountLower = validTotalAmount._1\n",
        "    val totalAmountUpper = validTotalAmount._2\n",
        "    \n",
        "    // Add month-date from data and compare it with month-date extracted from data file name\n",
        "    val transformedDF = renamedDF.withColumn(\"year_month\", regexp_extract(col(\"file_path\"), pattern.toString, 1)).withColumn(\"data_year_month\", concat_ws(\"-\", year(col(\"pickup_datetime\")), format_string(\"%02d\", month(col(\"pickup_datetime\")))))\n",
        "    \n",
        "    // Remove rows which follow rules\n",
        "    val filteredDF = transformedDF.filter(col(\"passenger_count\") =!= 0).filter(col(\"pulocationID\") =!= col(\"dolocationID\")).filter(col(\"data_year_month\") === col(\"year_month\")).filter(col(\"payment_type\").isin(validPaymentTypes: _*)).filter(col(\"RatecodeID\").isin(validRateCodeIds: _*)).filter(col(\"fare_amount\") =!= 0).filter(col(\"total_amount\").between(totalAmountLower, totalAmountUpper))\n",
        "    \n",
        "    // Impute: cast to Long\n",
        "    val castDf = filteredDF.withColumn(\"pulocationID\", col(\"pulocationID\").cast(LongType)).withColumn(\"dolocationID\", col(\"dolocationID\").cast(LongType))\n",
        "    castDf.select(\"pulocationID\", \"dolocationID\").coalesce(1)\n",
        "}\n",
        "  \n",
        " \n",
        "def ETLGreenTaxiTripDataset(spark: SparkSession, path: String, cleanOutputPath:String): Unit = {\n",
        "    val years = Array(\"2020\", \"2021\", \"2022\", \"2023\")\n",
        "    val months = Array(\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\")\n",
        "    \n",
        "    val filePaths2020 = for (m <- months.slice(9, 12)) yield s\"$path/2020/$m/green_tripdata_2020-${m}\"\n",
        "    val filePaths2021 = for (m <- months) yield s\"$path/2021/${m}/green_tripdata_2021-${m}\"\n",
        "    val filePaths2022 = for (m <- months) yield s\"$path/2022/${m}/green_tripdata_2022-${m}\"\n",
        "    val filePaths2023 = for (m <- months.slice(0, 9)) yield s\"$path/2023/${m}/green_tripdata_2023-${m}\"\n",
        "    \n",
        "    val filePaths = filePaths2020 ++ filePaths2021 ++ filePaths2022 ++ filePaths2023\n",
        "    \n",
        "    val pattern = \"\"\"(\\d{4}-\\d{2})\"\"\".r\n",
        "    \n",
        "    val schema = StructType(Array(StructField(\"pulocationID\", LongType, true), StructField(\"dolocationID\", LongType, true)))\n",
        "    var resultDF = spark.createDataFrame(spark.sparkContext.emptyRDD[Row], schema)\n",
        "\n",
        "    for (filePath <- filePaths.slice(0, 1)) {\n",
        "        val rawDF = loadRawTLCDataParquet(spark, filePath)\n",
        "        val transformedDF = rawDF.withColumn(\"file_path\", lit(filePath))\n",
        "        val cleanDF = cleanRawData(transformedDF)\n",
        "        resultDF = resultDF.union(cleanDF)\n",
        "    }\n",
        "    \n",
        "    saveCleanData(resultDF, cleanOutputPath)\n",
        "    loadCleanData(cleanOutputPath).show()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "ETLGreenTaxiTripDataset(spark, \"/user/cg4177_nyu_edu/project/data/source/tlc/green\", \"/user/cg4177_nyu_edu/project/data/clean/tlc/green\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "val tlcGreenCleanDF = loadCleanData(\"/user/cg4177_nyu_edu/project/data/clean/tlc/green\")\n",
        "tlcGreenCleanDF.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    },
    "name": "tlc_profiling"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
